---
title: Modelling Volatility
---

:::{.callout-warning}
This page will probably be broken up into a theory/methodology part and a case-study/implementation part in the future.
:::

# Overview

On this page, we want to show with a practical example how volatility in a portfolio can be modeled. When looking at historical returns of stocks, it's apparent that volatility is not constant over time, but is clustered. So during times of market distress, like the financial crisis

:::{.callout-note}
# Source
This page is largely based on the book "Elements of Financial Risk Management" by Peter F. Christoffersen [link](https://doi.org/10.1016/B978-0-12-174232-4.X5000-4). The code for the implementation is written by myself.
:::

# Setup

As before, we start by loading the packages that will be used.

```{r}
#| warning: false
#| message: false

library(tidyverse)
library(patchwork)
library(tidyquant)
library(forecast)
library(rugarch)

ggplot2::theme_set(theme_bw()) # Set a clean theme for plots
```

Now, we define the portfolio. We'll use the same five stocks for our portfolio as in the [VaR Case Study](case_study.qmd): Apple, Exxon Mobil, UnitedHealthCare, JP Morgan and Costco. Again, they receive an equal weight in the portfolio.

```{r}
stocks <- c("AAPL", "XOM", "UNH", "JPM", "COST")
weights_df <- tibble(symbol = stocks, weight = rep(1 / length(stocks), length(stocks)))
start_date <- ymd("2025-03-20") - (6 * 365) # Go back 6 years
end_date <- ymd("2025-03-20")

stock_data <- tq_get(stocks, from = start_date, to = end_date)

stock_returns <- stock_data %>%
  select(symbol, date, adjusted) %>%
  group_by(symbol) %>% # Calculate returns for each stock separately
  mutate(return = (adjusted - lag(adjusted)) / lag(adjusted)) %>% # Daily return
  slice_tail(n = -1) %>% # Remove the first row (no return for the first day)
  ungroup()

portfolio_returns <- stock_returns %>%
  left_join(weights_df, by = join_by(symbol)) %>% # Combine with weights
  group_by(date) %>% # Calculate portfolio return for each day
  summarise(return = weighted.mean(return, weight)) # Weighted average return

portfolio_returns_vec <- portfolio_returns$return
```

# Autocorrelation

In the following, we will assume the daily returns to be normally distributed with an expected value of zero and some variance $\sigma_t^2$. Note the subscript $t$ here, which means the variance is time-varying. We can then write $r_t = \sigma_t z_t$ where $z \sim N(0,1)$.

These assumptions, especially $\mathbb{E}(r_t)=0$, work well in the short term. Only for the long term, this would not be a great assumption because with this, stock markets would not rise over time. We can also test this assumption using a t-test. When looking at the last 100 returns, we find no statistically significant deviation from this assumption. But for 500 days we do.

```{r}
t.test(tail(portfolio_returns_vec, 100))
t.test(tail(portfolio_returns_vec, 500))
```

This assumption allows us to calculate the variance as the returns squared, because 
$$
\begin{aligned}
\mathbb{E}(r_t^2) &= \mathbb{E}(\sigma_t^2z_t^2)\\
&= \sigma_t^2 \mathbb{E}(z_t^2) \\
&= \sigma_t^2 (\text{Var}(z_t)+\mathbb{E}(z_t)^2) \\
&= \sigma_t^2 (1+0^2) \\
&= \sigma_t^2
\end{aligned}
$$
where we used the fact that $\sigma_t^2$ is deterministic and the rearranged variance decomposition to calculate $\mathbb{E}(z_t^2)$. The latter might seem unintuitive at first, but it can be checked easily in R using `mean(rnorm(1000)^2)`, which in fact yields (approximately) 1.

It is important to remember that this calculated variance is a noisy estimate based on our assumptions, serving as a proxy for the true, unobservable variance.

The variance can be visualized using a so-called ACF (Auto-Correlation-Function) plot and makes it apparent, that the variance of today's returns is very much dependent on the past variance. The dashed line shows that all lags up to around 25 days are statistically significant. The PACF (Partial-ACF), which shows the correlation between a time series and its lags that is not explained by previous lags, shows the first two lags to be especially significant.

```{r}
portfolio_variance_vec <- portfolio_returns_vec^2
forecast::ggAcf(portfolio_variance_vec) + forecast::ggPacf(portfolio_variance_vec)
```

```{r}
#| eval: false
#| include: false

# arima(portfolio_returns_vec^2, order = c(1, 0, 1))

pq_values <- expand.grid(p = 1:10, q = 1:10)

best_bic <- Inf
best_model <- NULL

for (i in 1:nrow(pq_values)) {
  p <- pq_values[i, 1]
  q <- pq_values[i, 2]

  # print(paste("p: ", p, " q: ", q, sep = ""))

  arma_model <- arima(portfolio_variance_vec, order = c(p, 0, q))
  bic <- BIC(arma_model)

  if (bic < best_bic) {
    best_p <- p
    best_q <- q
    best_bic <- bic
    best_model <- arma_model
  }
}

paste("Best p: ", best_p, " | Best q: ", best_q, sep = "")
```

# Moving Average Models
## Simple Moving Average

We could try and model the variance using a simple moving average of the $m$ past days.

$$
\sigma_t^2 = \frac{1}{m} \sum_{\tau=1}^m r_{t-\tau}^2
$$

The choice of $m$ is somewhat arbitrary, but we'll use $m=25$, because thats the number of statistically significant lags in the above ACF plot.

The result already looks quite good. While (of course) a lot smoother than the observed variances, we can clearly see the turbulences in 2022 and the calmer period in 2023-2024.

```{r}
#| warning: false

m <- 25

# predicted_var <- numeric(length(portfolio_variance_vec))
#
# for(i in 1:length(portfolio_variance_vec)) {
#   m_variances <- portfolio_variance_vec[max(i-m, 1):i]
#   predicted_var[i] <- 1/m * sum(m_variances)
# }

portfolio_returns %>%
  mutate(variance = return^2) %>%
  mutate(variance_pred = 1 / m * zoo::rollsum(variance, m, na.pad = TRUE)) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance")) %>%
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free")
```

## Exponential Smoothing

With the moving average, there is the implicit assumption that all $m$ past variances have the same impact on todays variance. This is probably not the case as can be seen in the ACF plot which shows a decreasing autocorrelation. That's why we'll look at exponential smoothing next:

$$
\sigma_{t}^2 = (1-\lambda) \sum_{\tau=1}^\infty \lambda^{\tau-1} r_{t-\tau}^2
$$
with $0 < \lambda < 1$

We start with $\lambda = 0.94$, because its suggested in "Elements of Financial Risk Management" by Peter F. Christoffersen. I implement the sum using a for loop. This might not be the most elegant approach, but it works well enough for this use case.

```{r}
f_pred_var <- function(lambda = 0.94) {
  predicted_var <- numeric(length(portfolio_variance_vec))

  for (i in 1:length(portfolio_variance_vec)) {
    # Previous squared returns, in "descending" order
    previous_variances <- portfolio_variance_vec[max(i - 1, 0):1]
    # Calculate lambdas for sum
    lambdas <- lambda^(1:max(i - 1, 0))
    # Calculate the actual sum
    predicted_var[i] <- (1 - lambda) * sum(lambdas * previous_variances)
  }

  predicted_var
}

portfolio_returns %>%
  mutate(variance = portfolio_variance_vec) %>%
  mutate(variance_pred = f_pred_var(0.94)) %>%
  filter(date > ymd("2020-12-31")) %>%
  ggplot(aes(x = date, y = variance_pred)) +
  geom_line()
```

Now find best fitting $\lambda$. For that, we repackage the code from above into a function with $\lambda$ as its input. But now, the return value is the sum of squared residuals (SSR) between the predicted variances and observed variances. This allows us to measure the difference between the actual and predicted values, where lower values show a better fit.

:::{.callout-note}
# SSR vs MLE
I currently minimize a SSR loss function. It seems to work, but is not the "standard" and has some drawbacks. For example, it's sensitive to outliers and assumes an incorrect error structure. Maximum Likelihood Estimation (MLE) would probably be better, but I have not yet come around to implementing it. In the meantime, I just hope the SSR results are not (heavily) biased.
:::

With that, we can construct a sequence of lambdas from 0 to 1, calculate the SSR for each and then plot the results to get a first feel for where the optimal $\lambda$ might lie.

```{r}
f_pred_var_ssr <- function(lambda) {
  predicted_var <- f_pred_var(lambda)
  sum((predicted_var - portfolio_variance_vec)^2)
}

lambda_grid_search <- tibble(lambda = seq(0.01, 0.99, by = 0.03)) %>%
  mutate(ssr = map_dbl(lambda, f_pred_var_ssr))

lambda_grid_search %>%
  ggplot(aes(x = lambda, y = ssr)) +
  geom_line()
```

The function seems to be continuous, convex and has a minimum (what we're after) $\lambda$ at around 0.75. The exact value can be found using `optim()` or `optimize()`. The resulting plot looks more "rugged" than before, which is expected because the "smoothing" influence of past returns decays more quickly with a smaller $\lambda$.

```{r}
lambda_optim <- optimize(f_pred_var_ssr, interval = c(0, 1))
lambda_optim$minimum

portfolio_returns %>%
  mutate(
    variance = portfolio_variance_vec,
    variance_pred = f_pred_var(0.94),
    variance_pred_opt = f_pred_var(lambda_optim$minimum)
  ) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance_pred")) %>%
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free")
```

With this approach and a smaller $\lambda$, we can cut down the computational cost by not summing over all past returns, but only a subset. Because the weight $\lambda^{\tau-1}$ approaches zero fast, we can neglect all values except, for example, the past 30 days to get basically the same results. Using $\lambda = 0.75$, the past 50 days account for nearly 100% of the weights.

```{r}
(1 - 0.75) * sum(0.75^(0:29))
```

The exponential smoothing model can also be used for forecasting the variance, here with $\lambda=0.75$.

```{r}
last_30_variances <- rev(tail(portfolio_variance_vec, 30))

variance_tomorrow <- (1 - 0.75) * sum(0.75^(0:29) * last_30_variances)

portfolio_returns %>%
  mutate(variance = portfolio_variance_vec) %>%
  slice_tail(n = 20) %>%
  ggplot(aes(x = date, y = variance)) +
  geom_col() +
  geom_col(
    data = tibble(date = end_date + 1, variance_pred = variance_tomorrow),
    aes(y = variance_pred), fill = "red"
  )
```

# GARCH

GARCH stands for autoregressive conditional heteroskedasticity. In genral, $\text{GARCH}(p,q)$ is defined as
$$
\sigma_t^2 = \omega + \sum_{i=1}^{q} \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^{p} \beta_j \sigma_{t-j}^2
$$
but we will focus on the simpler, but still powerful and most often used case of $\text{GARCH}(1,1)$. Moreover, because we assume $\mathbb{E}(r_t)=0$, we can use the return $r_t$ directly instead of the residual $\epsilon_t$. Thus, our model will be
$$
\sigma_t^2 = \omega + \alpha r_{t-1}^2 + \beta \sigma_{t-1}^2
$$
with $\alpha + \beta < 1$.

We can define the long-run average variance, also called unconditional variance, as $\sigma^2$ (without the subscript $t$) and rewrite the formula as
$$
\sigma_t^2 = \sigma^2 + \alpha (r_{t-1}^2 - \sigma^2) + \beta (\sigma_{t-1}^2 - \sigma^2)
$$
For more details, see chapter 3 in "Elements of Financial Risk Management" by Peter F. Christoffersen.

## Simulation

To first get a feel for this kind of model, it can be helpful to generate some data. That way, we can specify everything and then check if we can estimate these parameters again. We'll use $\omega = 1.1\cdot 10^{-6}$, $\alpha=0.1$ and $\beta=0.899$. Note that this model is very sensitive to the parameters.

```{r}
set.seed(3)

garch_sim_n <- 1000
garch_sim_variance <- numeric(garch_sim_n)
garch_sim_returns <- numeric(garch_sim_n)

garch_sim_omega <- 1.1e-06
garch_sim_alpha <- 0.1
garch_sim_beta <- 0.899

garch_sim_variance[1] <- 0
garch_sim_returns[1] <- 0.01

for (t in 2:length(garch_sim_returns)) {
  garch_sim_variance[t] <- garch_sim_omega +
    garch_sim_alpha * garch_sim_returns[t - 1]^2 +
    garch_sim_beta * garch_sim_variance[t - 1]
  garch_sim_returns[t] <- sqrt(garch_sim_variance[t]) * rnorm(1)
}

garch_sim_results <- tibble(
  t = 1:garch_sim_n,
  return = garch_sim_returns,
  variance = garch_sim_variance
)

head(garch_sim_results)
```

Looking at the resulting time series, we see that it indeed resembles the movement of a stock price quite well, with "calmer" times (at the start and around $t=750$ for example) and large and somewhat persistant volatility spikes (especially at around $t=500$).

We also see, that the squared returns are indeed a pretty good, although noisy proxy for the (in the real world unobserved) variance.

```{r}
garch_sim_p1 <- garch_sim_results %>%
  mutate(return_cum = cumprod(1 + return)) %>%
  ggplot(aes(x = t, y = return_cum)) +
  geom_line()

garch_sim_p2 <- garch_sim_results %>%
  ggplot(aes(x = t)) +
  geom_line(aes(y = return^2))

garch_sim_p3 <- garch_sim_results %>%
  ggplot(aes(x = t)) +
  geom_line(aes(y = variance))

garch_sim_p1 + (garch_sim_p2 / garch_sim_p3)
```

Fitting a GARCH(1,1) model using the `{rugarch}` package. The result for $\omega$ is not statistically significant, but for $\alpha$ and $\beta$ they are and also quite close to the true values. So we can indeed recover the parameters.

Note that the output from this GARCH model also includes `mu`, `ar1` and `ba1` estimates. These stem from a ARMA model for the mean return. We assumed that the mean return is 0 ($\mathbb{E}(r_t)=0$) and thus did not care about modelling the returns. And because we also did not built any process for the returns into our simulation, all the parameters here are not statistically significant. We could add `mean.model = list(armaOrder = c(0, 0), include.mean = FALSE)` to the model specification to explicitly disable the ARMA process.

```{r}
garch_sim_spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)))
garch_sim_fit <- ugarchfit(garch_sim_spec, data = garch_sim_returns)

garch_sim_fit@fit$matcoef %>%
  round(4)
```

We can also try and do this ourselves. While the Maximum-Likelihood-Estimator (MLE) is generally preferred for this kind of model, we also will take a look at a approach for minimizing the Sum of Squared Residuals (SSR). The following code implements the GARCH model as a function, defines functions to minimize and then uses `optim()` to (try to) find the minimum and thus the optimal parameters.

:::{.callout-note}
# Maximum-Likelihood-Estimation
*We will not go in depth on this.* MLE is a method for estimating parameters for a distribution. The key idea is to maximize the likelihood of observing the data given a set of parameters:
$$
\mathcal{L}(\mathbf{\theta}; \mathbf{x}) = \prod_{i=1}^n f(x_i; \mathbf{\theta})
$$
It is often more convenient to work with the logarithm of the likelihood function, called the log-likelihood function:
$$
\ell(\mathbf{\theta}; \mathbf{x}) = \sum_{i=1}^n \log f(x_i; \mathbf{\theta})
$$
:::

```{r}
f_garch <- function(omega, alpha, beta, returns_vec = portfolio_returns_vec) {
  predicted_var_garch <- numeric(length(returns_vec))
  predicted_var_garch[1] <- omega / (1 - alpha - beta) # Long-run variance

  for (i in 2:length(returns_vec)) {
    predicted_var_garch[i] <- omega +
      alpha * returns_vec[i - 1]^2 +
      beta * predicted_var_garch[i - 1]
  }

  predicted_var_garch
}

f_garch_ssr <- function(params, returns_vec, variances_vec) {
  omega <- params[1]
  alpha <- params[2]
  beta <- params[3]

  # Enforce constraints
  if((alpha + beta) >= 1) return(1e10)

  predicted_var <- f_garch(omega, alpha, beta, returns_vec)

  # Sum of Squared Residuals (to minimize)
  sum((predicted_var - variances_vec)^2)
}

f_garch_mle <- function(params, returns_vec) {
  omega <- params[1]
  alpha <- params[2]
  beta <- params[3]

  # Enforce constraints
  if((alpha + beta) >= 1) return(1e10)

  predicted_var <- f_garch(omega, alpha, beta, returns_vec)

  # Negative log-likelihood (to minimize)
  0.5 * sum(log(predicted_var) + (returns_vec^2) / predicted_var)
}

garch_sim_optim_ssr <- optim(
  par = c(0.001, 0.29, 0.7),
  fn = f_garch_ssr,
  returns_vec = garch_sim_returns,
  variances_vec = garch_sim_returns^2,
  # Optimizing with bounds
  method = "L-BFGS-B",
  lower = c(1e-9, 1e-6, 1e-6),
  upper = c(Inf, 0.999, 0.999)
)
# garch_sim_optim_ssr$par

garch_sim_optim_mle <- optim(
  par = c(0.001, 0.29, 0.7),
  fn = f_garch_mle,
  returns_vec = garch_sim_returns,
  # Optimizing with bounds
  method = "L-BFGS-B",
  lower = c(1e-12, 1e-6, 1e-6),
  upper = c(Inf, 0.999, 0.999)
)
# garch_sim_optim_mle$par
```

Here are the results. As we can see, they are not great for our own optimization. MLE just took the initial values for the optimization. And SSR is even further from the true values. In general, with this kind of problem the optimization is very sensitive to the initial values.

```{r}
tibble(
  parameter = c("omega", "alpha", "beta"),
  true = c(garch_sim_omega, garch_sim_alpha, garch_sim_beta),
  rugarch = unname(coef(garch_sim_fit)[c("omega", "alpha1", "beta1")]),
  ssr = garch_sim_optim_ssr$par,
  mle = garch_sim_optim_mle$par
) %>%
  mutate(across(where(is.numeric), ~ round(.x, 4)))
```

We can just try a brute force approach many different parameter combinations and see which one fits best. The following code applies about 250,000 parameter combinations. We could reduce the number of combinations by setting a lower bound as well. If we know for example from past experience, that $\alpha + \beta \geq 0.8$, we only have about 90.000 combinations left.

The accuracy can easily be improved by trying more parameter combinations (by setting `length.out = 500` for example). But then, the computational load increases dramatically.

```{r}
#| cache: true

garch_sim_grid_search <- expand_grid(
  omega = seq(0, 0.0001, length.out = 50),
  alpha = seq(0.01, 0.99, length.out = 100),
  beta = seq(0.01, 0.99, length.out = 100)
) %>%
  filter(alpha + beta < 1, alpha + beta >= 0.8) %>%
  mutate(
    ssr = pmap_dbl(
      list(omega, alpha, beta),
      ~ f_garch_ssr(
        params = c(..1, ..2, ..3),
        returns_vec = garch_sim_returns,
        variances_vec = garch_sim_returns^2
      )
    ),
    mle = pmap_dbl(
      list(omega, alpha, beta),
      ~ f_garch_mle(
        params = c(..1, ..2, ..3),
        returns_vec = garch_sim_returns
      )
    )
  )
```

And the recovered parameters here are actually better than with `rugarch`.

```{r}
# Best parameters with SSR
garch_sim_grid_search %>%
  slice_min(n = 1, order_by = ssr) %>%
  select(omega, alpha, beta) %>%
  unlist() %>%
  round(4)

# Best parameters with MLE
garch_sim_grid_search %>%
  slice_min(n = 1, order_by = mle) %>%
  select(omega, alpha, beta) %>%
  unlist() %>%
  round(4)
```

## Real Data

Now, lets look again at our real-world portfolio. As we have seen above, we can get quite good results using the `rugarch` package, so that's what we will do here.

When looking at the results from the model, we might first be interested in checking our assumption $\mathbb{E}=0$. While `mu` is statistically significant here, it is still close to zero. The `ar1` and `ma1` parameters are still not statistically significant (to the 5% level), which is good news for us. That means, that while the returns don't really have an expected value of zero, they at least are independent from each other.

Plotting the model, we can see that the GARCH model looks quite similar to the Exponential Smoothing Model. And our variance proxy (using the squared returns) is again the most noisy.

```{r}
garch_spec <- ugarchspec(
  variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
  # mean.model = list(armaOrder = c(0, 0), include.mean = FALSE) # Add this for restriction
)
garch_fit <- ugarchfit(garch_spec, data = portfolio_returns_vec)

garch_fit@fit$matcoef %>%
  round(4)

portfolio_returns %>%
  mutate(
    variance_proxy = portfolio_returns_vec^2,
    variance_pred_exp_smoothing = f_pred_var(0.73),
    variance_pred_garch = f_garch(
      coef(garch_fit)["omega"], coef(garch_fit)["alpha1"], coef(garch_fit)["beta1"]
    ),
  ) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance")) %>%
  ggplot(aes(x = date, y = value, color = name)) +
  geom_line() +
  theme(legend.position = "none")+
  facet_wrap(~name)
```

## Forecasting

Now that we have a solid model, we can try to forecast the variance and returns for the next, in this example, 100 days. The red line in the plots show the fitted values for return and variance, while the blue line shows the forecast.

We can see that the (cumulated) return forecast is going straight up. Thats because in our `rugarch` model, the `mu` is not zero. If it was or if we had set the restriction, we would see a horizontal line here.

For the variance, we see that it is going down and approaching the long-run variance $\sigma^2 = \frac{\omega}{1 - \alpha - \beta}$.

```{r}
forecast_horizon <- 100
garch_forecast <- ugarchforecast(garch_fit, n.ahead = forecast_horizon)

portfolio_returns_predicted <- tibble(
  date = seq(
    from = max(portfolio_returns$date)+1,
    to = max(portfolio_returns$date)+forecast_horizon,
    by = "1 day"
    ),
  return = garch_forecast@forecast$seriesFor,
  variance = (garch_forecast@forecast$sigmaFor)^2,
  type = "prediction"
)

portfolio_returns_combined <- portfolio_returns %>%
  mutate(variance = garch_fit@fit$var,
         type = "fitted") %>%
  rbind(portfolio_returns_predicted)

portfolio_returns_combined %>%
  tail(500) %>%
  ggplot(aes(x = date, y = variance, color = type))+
  geom_line()

portfolio_returns_combined %>%
  tail(500) %>%
  mutate(return_cum = cumprod(1+return), .keep = "unused") %>%
  pivot_longer(cols = c(return_cum, variance)) %>%
  ggplot(aes(x = date, y = value, color = type))+
  geom_line()+
  facet_wrap(~name, ncol = 2, scales = "free")+
  theme(legend.position = "top")
```

## GARCH(2,2)

Looking back at the PACF plot from earlier, we saw that the first *two* lags were significant. But until now, we only used the first lag with our $\text{GARCH}(1,1)$ model. Let's change that now and estimate a $\text{GARCH}(2,2)$ model:

$$
\sigma_t^2 = \omega + \alpha_1 r_{t-1}^2 + \alpha_2 r_{t-2}^2 + \beta_1 \sigma_{t-1}^2 + \beta_2 \sigma_{t-1}^2
$$

We can see, that the ARMA part looks like before, but now we get `alpha1` and `alpha2` as well as `beta1` and `beta2`. Of these, only `beta1` is not statistically significant. That suggests, that a $\text{GARCH}(2,2)$ is a good fit for our data.

```{r}
garch_22_spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 2)))
garch_22_fit <- ugarchfit(garch_22_spec, data = portfolio_returns_vec)

garch_22_fit@fit$matcoef %>%
  round(4)
```
