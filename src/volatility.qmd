---
title: Modelling Volatility
---

# Overview

On this page, we want to show with a practical example how volatility in a portfolio can be modeled. When looking at historical returns of stocks, it's apparent that volatility is not constant over time, but is clustered. So during times of market distress, like the financial crisis

:::{.callout-note}
# Source
This page is largely based on the book "Elements of Financial Risk Management" by Peter F. Christoffersen [link](https://doi.org/10.1016/B978-0-12-174232-4.X5000-4). The code for the implementation is written by myself.
:::

# Setup

As before, we start by loading the packages that will be used.

```{r}
#| warning: false
#| message: false

library(tidyverse)
library(patchwork)
library(tidyquant)
library(forecast)
library(rugarch)

ggplot2::theme_set(theme_bw()) # Set a clean theme for plots
```

Now, we define the portfolio. We'll use the same five stocks for our portfolio as in the ![VaR Case Study](case_study.qmd): Apple, Exxon Mobil, UnitedHealthCare, JP Morgan and Costco. Again, they receive an equal weight in the portfolio.

```{r}
stocks <- c("AAPL", "XOM", "UNH", "JPM", "COST")
weights_df <- tibble(symbol = stocks, weight = rep(1 / length(stocks), length(stocks)))
start_date <- ymd("2025-03-20") - (6 * 365) # Go back 6 years
end_date <- ymd("2025-03-20")

stock_data <- tq_get(stocks, from = start_date, to = end_date)

stock_returns <- stock_data %>%
  select(symbol, date, adjusted) %>%
  group_by(symbol) %>% # Calculate returns for each stock separately
  mutate(return = (adjusted - lag(adjusted)) / lag(adjusted)) %>% # Daily return
  slice_tail(n = -1) %>% # Remove the first row (no return for the first day)
  ungroup()

portfolio_returns <- stock_returns %>%
  left_join(weights_df, by = join_by(symbol)) %>% # Combine with weights
  group_by(date) %>% # Calculate portfolio return for each day
  summarise(return = weighted.mean(return, weight)) # Weighted average return

portfolio_returns_vec <- portfolio_returns$return
```

# Autocorrelation

In the following, we will assume the daily returns to have an expected value of 0 and are normally distributed, so that we can write $r_t = \sigma_t z_t$ where $z \sim N(0,1)$. This is approximately the case for shorter time periods. And even for longer periods, the expected return is very small.

```{r}
t.test(tail(portfolio_returns_vec, 100))
t.test(tail(portfolio_returns_vec, 500))
```

This assumption allows us to calculate the variance as the returns squared, because with $\mu=0$

$$
\begin{aligned}
\rho_k &= \frac{\gamma_k}{\gamma_0} \\
&= \frac{\operatorname{E}[(r_t - \mu)(r_{t-k} - \mu)]}{\operatorname{E}[(r_t - \mu)^2]} \\
&= \frac{\operatorname{E}[r_t r_{t-k}]}{\operatorname{E}[r_t^2]}
\end{aligned}
$$
where $\rho_k$ is the autocorrelation at lag $k$. 

This can be visualized using a so called ACF plot and makes it apparent, that the variance of todays returns are very much dependent on the past variance. The dashed line shows that all lags up to around 25 days are statistically signifikant.

```{r}
portfolio_variance_vec <- portfolio_returns_vec^2
forecast::ggAcf(portfolio_variance_vec)
```

```{r}
#| eval: false
#| include: false

# arima(portfolio_returns_vec^2, order = c(1, 0, 1))

pq_values <- expand.grid(p = 1:10, q = 1:10)

best_bic <- Inf
best_model <- NULL

for (i in 1:nrow(pq_values)) {
  p <- pq_values[i, 1]
  q <- pq_values[i, 2]

  # print(paste("p: ", p, " q: ", q, sep = ""))

  arma_model <- arima(portfolio_variance_vec, order = c(p, 0, q))
  bic <- BIC(arma_model)

  if (bic < best_bic) {
    best_p <- p
    best_q <- q
    best_bic <- bic
    best_model <- arma_model
  }
}

paste("Best p: ", best_p, " | Best q: ", best_q, sep = "")
```

# Moving Average

We could try and model the variance using a simple moving average of the $m$ past days.

$$
\sigma_t^2 = \frac{1}{m} \sum_{\tau=1}^m r_{t-\tau}^2
$$

The choice of $m$ is somewhat arbitrary, but we'll use $m=25$, because thats the number of statistically significant lags in the above ACF plot.

The result already looks quite good. While (of course) a lot smoother than the observed variances, we can clearly see the turbulences in 2022 and the calmer period in 2023-2024.

```{r}
#| warning: false

m <- 25

# predicted_var <- numeric(length(portfolio_variance_vec))
#
# for(i in 1:length(portfolio_variance_vec)) {
#   m_variances <- portfolio_variance_vec[max(i-m, 1):i]
#   predicted_var[i] <- 1/m * sum(m_variances)
# }

portfolio_returns %>%
  mutate(variance = return^2) %>%
  mutate(variance_pred = 1 / m * zoo::rollsum(variance, m, na.pad = TRUE)) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance")) %>%
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free")
```


# Exponential Smoothing

With the moving average, there is the implicit assumption that all $m$ past variances have the same impact on todays variance. This is probably not the case as can be seen in the ACF plot which shows a decreasing autocorrelation. That's why we'll look at exponential smoothing next:

$$
\sigma_{t}^2 = (1-\lambda) \sum_{\tau=1}^\infty \lambda^{\tau-1} r_{t-\tau}^2
$$
with $0 < \lambda < 1$

We start with $\lambda = 0.94$, because its suggested in "Elements of Financial Risk Management" by Peter F. Christoffersen. I implement the sum using a for loop. This might not be the most elegant approach, but it works well enough for this use case.

```{r}
f_pred_var <- function(lambda = 0.94) {
  predicted_var <- numeric(length(portfolio_variance_vec))

  for (i in 1:length(portfolio_variance_vec)) {
    # Previous squared returns, in "descending" order
    previous_variances <- portfolio_variance_vec[max(i - 1, 0):1]
    # Calculate lambdas for sum
    lambdas <- lambda^(1:max(i - 1, 0))
    # Calculate the actual sum
    predicted_var[i] <- (1 - lambda) * sum(lambdas * previous_variances)
  }

  predicted_var
}

portfolio_returns %>%
  mutate(variance = portfolio_variance_vec) %>%
  mutate(variance_pred = f_pred_var(0.94)) %>%
  filter(date > ymd("2020-12-31")) %>%
  ggplot(aes(x = date, y = variance_pred)) +
  geom_line()
```

Now find best fitting $\lambda$. For that, we repackage the code from above into a function with $\lambda$ as its input. But now, the return value is the sum of squared residuals (SSR) between the predicted variances and observed variances. This allows us to measure the difference between the actual and predicted values, where lower values show a better fit.

:::{.callout-note}
# SSR vs MLE
I currently minimize a SSR loss function. It seems to work, but is not the "standard" and has some drawbacks. For example, it's sensitive to outliers and assumes an incorrect error structure. Maximum Likelihood Estimation (MLE) would probably be better, but I have not yet come around to implementing it. In the meantime, I just hope the SSR results are not (heavily) biased.
:::

With that, we can construct a sequence of lambdas from 0 to 1, calculate the SSR for each and then plot the results to get a first feel for where the optimal $\lambda$ might lie.

```{r}
f_pred_var_ssr <- function(lambda) {
  predicted_var <- f_pred_var(lambda)
  sum((predicted_var - portfolio_variance_vec)^2)
}

lambda_grid_search <- tibble(lambda = seq(0.01, 0.99, by = 0.03)) %>%
  mutate(ssr = map_dbl(lambda, f_pred_var_ssr))

lambda_grid_search %>%
  ggplot(aes(x = lambda, y = ssr)) +
  geom_line()
```

The function seems to be continuous, convex and has a minimum (what we're after) $\lambda$ at around 0.75. The exact value can be found using `optim()` or `optimize()`. The resulting plot looks more "rugged" than before, which is expected because the "smoothing" influence of past returns decays more quickly with a smaller $\lambda$.

```{r}
lambda_optim <- optimize(f_pred_var_ssr, interval = c(0, 1))
lambda_optim$minimum

portfolio_returns %>%
  mutate(
    variance = portfolio_variance_vec,
    variance_pred = f_pred_var(0.94),
    variance_pred_opt = f_pred_var(lambda_optim$minimum)
  ) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance_pred")) %>%
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free")
```

With this approach and a smaller $\lambda$, we can cut down the computational cost by not summing over all past returns, but only a subset. Because the weight $\lambda^{\tau-1}$ approaches zero fast, we can neglect all values except, for example, the past 30 days to get basically the same results. Using $\lambda = 0.75$, the past 50 days account for nearly 100% of the weights.

```{r}
(1 - 0.75) * sum(0.75^(0:29))
```

The exponential smoothing model can also be used for forecasting the variance, here with $\lambda=0.75$.

```{r}
last_30_variances <- rev(tail(portfolio_variance_vec, 30))

variance_tomorrow <- (1 - 0.75) * sum(0.75^(0:29) * last_30_variances)

portfolio_returns %>%
  mutate(variance = portfolio_variance_vec) %>%
  slice_tail(n = 20) %>%
  ggplot(aes(x = date, y = variance)) +
  geom_col() +
  geom_col(
    data = tibble(date = end_date + 1, variance_pred = variance_tomorrow),
    aes(y = variance_pred), fill = "red"
  )
```


# GARCH

GARCH stands for autoregressive conditional heteroskedasticity. In genral, $\text{GARCH}(p,q)$ is defined as
$$
\sigma_t^2 = \omega + \sum_{i=1}^{q} \alpha_i \epsilon_{t-i}^2 + \sum_{j=1}^{p} \beta_j \sigma_{t-j}^2
$$
but we will focus on the simpler, but still powerful and most often used case of $\text{GARCH}(1,1)$. Moreover, because we assume $\mathbb{E}(r_t)=0$, we can use the return $r_t$ directly instead of the residual $\epsilon_t$. Thus, our model will be
$$
\sigma_t^2 = \omega + \alpha r_{t-1}^2 + \beta \sigma_{t-1}^2
$$
with $\alpha + \beta < 1$.

We can define the long-run average variance, also called unconditional variance, as $\sigma^2$ (without the subscript $t$) and rewrite the formula as
$$
\sigma_t^2 = \sigma^2 + \alpha (r_{t-1}^2 - \sigma^2) + \beta (\sigma_{t-1}^2 - \sigma^2)
$$
For more details, see chapter 3 in "Elements of Financial Risk Management" by Peter F. Christoffersen.

First, lets look at an example using values for $\omega$, $\alpha$ and $\beta$

```{r}
f_garch <- function(omega, alpha, beta, returns_vec = portfolio_returns_vec) {
  predicted_var_garch <- numeric(length(returns_vec))
  predicted_var_garch[1] <- omega / (1 - alpha - beta) # Long-run variance

  for (i in 2:length(returns_vec)) {
    predicted_var_garch[i] <- omega +
      alpha * returns_vec[i - 1]^2 +
      beta * predicted_var_garch[i - 1]
  }

  predicted_var_garch
}

portfolio_returns %>%
  mutate(
    # variance = portfolio_variance_vec,
    variance_pred_exp_smoothing = f_pred_var(0.94),
    variance_pred_garch = f_garch(0.0000011, 0.1, 0.899),
  ) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance")) %>%
  ggplot(aes(x = date, y = value, color = name)) +
  geom_line() +
  # scale_y_continuous(transform = scales::pseudo_log_trans(sigma = 0.00001)) +
  theme(legend.position = "top")
```

Now we want to find good values for our portfolio returns. For that, we will use SSR and MLE. MLE is generally preferred here, because it does only use the observed returns and not the variances (squared returns). *However, the MLE implementation is currently not working properly.*

:::{.callout-note}
# Maximum-Likelihood-Estimation
*We will not go in depth on this.* MLE is a method for estimating parameters for a distribution. The key idea is to maximize the likelihood of observing the data given a set of parameters:
$$
\mathcal{L}(\mathbf{\theta}; \mathbf{x}) = \prod_{i=1}^n f(x_i; \mathbf{\theta})
$$
It is often more convenient to work with the logarithm of the likelihood function, called the log-likelihood function:
$$
\ell(\mathbf{\theta}; \mathbf{x}) = \sum_{i=1}^n \log f(x_i; \mathbf{\theta})
$$
:::

```{r}
f_garch_ssr <- function(params, returns_vec, variances_vec) {
  omega <- params[1]
  alpha <- params[2]
  beta <- params[3]

  predicted_var <- f_garch(omega, alpha, beta, returns_vec)

  # Sum of Squared Residuals (to minimize)
  sum((predicted_var - variances_vec)^2)
}

f_garch_mle <- function(params, returns_vec) {
  omega <- params[1]
  alpha <- params[2]
  beta <- params[3]

  predicted_var <- f_garch(omega, alpha, beta, returns_vec)

  # Negative log-likelihood (to minimize)
  -0.5 * sum(log(predicted_var) + (returns_vec^2) / predicted_var)
}

garch_optim_ssr <- optim(
  par = c(0.0000011, 0.1, 0.899),
  fn = f_garch_ssr,
  returns_vec = portfolio_returns_vec,
  variances_vec = portfolio_variance_vec,
  # Optimizing with bounds fails
  # method = "L-BFGS-B",
  # lower = c(1e-9, 1e-6, 1e-6),
  # upper = c(Inf, 0.999, 0.999)
)
garch_optim_ssr$par

garch_optim_mle <- optim(
  par = c(0.0000011, 0.1, 0.899),
  fn = f_garch_mle,
  returns_vec = portfolio_returns_vec,
  # Optimizing with bounds fails
  # method = "L-BFGS-B",
  # lower = c(1e-12, 1e-6, 1e-6),
  # upper = c(Inf, 0.999, 0.999)
)
garch_optim_mle$par
```

There is also a dedicated packages for GARCH modelling in R, `{rugarch}`, which allows for a more robust estimation. It also uses a MLE approach under the hood.

```{r}
garch_spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)))
garch_fit <- ugarchfit(garch_spec, data = portfolio_returns_vec)
coef(garch_fit)
```

Now plot with the optimal GARCH parameters (from `{rugarch}`).

```{r}
portfolio_returns %>%
  mutate(
    # variance = portfolio_variance_vec,
    variance_pred_exp_smoothing = f_pred_var(0.94),
    variance_pred_garch = f_garch(0.0000011, 0.1, 0.899),
    variance_pred_garch_opt = f_garch(
      coef(garch_fit)["omega"], coef(garch_fit)["alpha1"], coef(garch_fit)["beta1"]
    ),
  ) %>%
  filter(date > ymd("2020-12-31")) %>%
  pivot_longer(starts_with("variance")) %>%
  ggplot(aes(x = date, y = value, color = name)) +
  geom_line() +
  # scale_y_continuous(transform = scales::pseudo_log_trans(sigma = 0.00001)) +
  theme(legend.position = "top")
```
